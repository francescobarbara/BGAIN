{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BGAIN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN6aHCE/o/CatDGXEOdck3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescobarbara/BGAIN/blob/main/BGAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olkOtHvOf8Rm",
        "outputId": "e897665d-0f02-4e32-cdef-11e0a6e7f567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GAIN'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 80 (delta 8), reused 0 (delta 0), pack-reused 58\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/francescobarbara/GAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "PM7zlXXjgKfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "VvV---_Bh1t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "##IF USING TF 2 use following import to still use TF < 2.0 Functionalities\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "def normalization (data, parameters=None):    #tutto ok, input semplice dataset\n",
        "  '''Normalize data \n",
        "  \n",
        "  Args:\n",
        "    - data: original data\n",
        "  \n",
        "  Returns:\n",
        "    - norm_data: normalized data\n",
        "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
        "  '''\n",
        "\n",
        "  # Parameters\n",
        "  _, dim = data.shape\n",
        "  norm_data = data.copy()\n",
        "  \n",
        "  if parameters is None:\n",
        "  \n",
        "    # MixMax normalization\n",
        "    mean = np.nanmean(data, axis = 0)\n",
        "    std = np.nanstd(data, axis = 0)\n",
        "    \n",
        "    # For each dimension\n",
        "    for i in range(dim):\n",
        "        \n",
        "      norm_data[:,i] = norm_data[:,i] - mean[i]\n",
        "      if std[i] != 0:\n",
        "          norm_data[:,i] = norm_data[:,i] / std[i]   \n",
        "      \n",
        "    # Return norm_parameters for renormalization\n",
        "    norm_parameters = {'mean': mean,\n",
        "                       'std': std}  \n",
        "      \n",
        "  return norm_data, norm_parameters\n",
        "\n",
        "def renormalization (norm_data, norm_parameters): #norm_pars is a dict\n",
        "  '''Renormalize data from [0, 1] range to the original range.\n",
        "  \n",
        "  Args:\n",
        "    - norm_data: normalized data\n",
        "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
        "  \n",
        "  Returns:\n",
        "    - renorm_data: renormalized original data\n",
        "  '''\n",
        "  \n",
        "  mean = norm_parameters['mean']\n",
        "  std = norm_parameters['std']\n",
        "\n",
        "  _, dim = norm_data.shape\n",
        "  renorm_data = norm_data.copy()\n",
        "    \n",
        "  for i in range(dim):\n",
        "    renorm_data[:,i] = renorm_data[:,i] * std[i] + mean[i]   \n",
        "    \n",
        "  return renorm_data\n",
        "\n",
        "\n",
        "def rounding (imputed_data, data_x):\n",
        "  '''Round imputed data for categorical variables.\n",
        "  \n",
        "  Args:\n",
        "    - imputed_data: imputed data\n",
        "    - data_x: original data with missing values\n",
        "    \n",
        "  Returns:\n",
        "    - rounded_data: rounded imputed data\n",
        "  '''\n",
        "  \n",
        "  _, dim = data_x.shape\n",
        "  rounded_data = imputed_data.copy()\n",
        "  \n",
        "  for i in range(dim):\n",
        "    temp = data_x[~np.isnan(data_x[:, i]), i]   \n",
        "    # Only for the categorical variable\n",
        "    if len(np.unique(temp)) < 20:\n",
        "      rounded_data[:, i] = np.round(rounded_data[:, i])\n",
        "      \n",
        "  return rounded_data\n",
        "\n",
        "\n",
        "def rmse_loss (ori_data, imputed_data, data_m):   #just takes two datasets as inputs\n",
        "  '''Compute RMSE loss between ori_data and imputed_data\n",
        "  \n",
        "  Args:\n",
        "    - ori_data: original data without missing values\n",
        "    - imputed_data: imputed data\n",
        "    - data_m: indicator matrix for missingness\n",
        "    \n",
        "  Returns:\n",
        "    - rmse: Root Mean Squared Error\n",
        "  '''\n",
        "  \n",
        "  ori_data, norm_parameters = normalization(ori_data)\n",
        "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
        "    \n",
        "  # Only for missing values\n",
        "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
        "  denominator = np.sum(1-data_m)\n",
        "  \n",
        "  rmse = np.sqrt(nominator/float(denominator))\n",
        "  \n",
        "  return rmse\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "  '''Xavier initialization.\n",
        "  \n",
        "  Args:\n",
        "    - size: vector size\n",
        "    \n",
        "  Returns:\n",
        "    - initialized random vector.\n",
        "  '''\n",
        "  in_dim = size[0]\n",
        "  xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "  return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
        "      \n",
        "\n",
        "def binary_sampler(p, rows, cols):  #creates matrix with entries Bernoulli p\n",
        "  '''Sample binary random variables.\n",
        "  \n",
        "  Args:\n",
        "    - p: probability of 1\n",
        "    - rows: the number of rows\n",
        "    - cols: the number of columns\n",
        "    \n",
        "  Returns:\n",
        "    - binary_random_matrix: generated binary random matrix.\n",
        "  '''\n",
        "  unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
        "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
        "  return binary_random_matrix\n",
        "\n",
        "\n",
        "def uniform_sampler(low, high, rows, cols):\n",
        "  '''Sample uniform random variables.\n",
        "  \n",
        "  Args:\n",
        "    - low: low limit\n",
        "    - high: high limit\n",
        "    - rows: the number of rows\n",
        "    - cols: the number of columns\n",
        "    \n",
        "  Returns:\n",
        "    - uniform_random_matrix: generated uniform random matrix.\n",
        "  '''\n",
        "  return np.random.uniform(low, high, size = [rows, cols])       \n",
        "\n",
        "\n",
        "def sample_batch_index(total, batch_size): #returns a number batch size of indices\n",
        "  '''Sample index of the mini-batch.\n",
        "  \n",
        "  Args:\n",
        "    - total: total number of samples\n",
        "    - batch_size: batch size\n",
        "    \n",
        "  Returns:\n",
        "    - batch_idx: batch index\n",
        "  '''\n",
        "  total_idx = np.random.permutation(total)\n",
        "  batch_idx = total_idx[:batch_size]\n",
        "  return batch_idx\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWfL0x8tgJzh",
        "outputId": "6366a0bb-755b-4214-8466-24d807de2652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New 3 methods here"
      ],
      "metadata": {
        "id": "QWwfVPaZggSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(n,d):\n",
        "  return np.random.uniform(size = (n,d))"
      ],
      "metadata": {
        "id": "tFwi05CLOr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sigma_1 (data_x, data_m): \n",
        "    \n",
        "    #data_m = np.array(data_m)\n",
        "    #data should already have been centered IMPORTANT\n",
        "    n, d = data_x.shape\n",
        "    sigma_1 = np.zeros((d, d))\n",
        "    \n",
        "    \n",
        "    \n",
        "    for i in range(d):\n",
        "        for j in range(d):\n",
        "            count = 0\n",
        "            for row in range(n):\n",
        "                if data_m[row, i] == 1 and data_m[row, j] == 1:\n",
        "                    sigma_1[i,j] += data_x[row, i] * data_x[row, j]\n",
        "                    count += 1\n",
        "                    \n",
        "            if count > 1:\n",
        "                sigma_1[i,j] = sigma_1[i,j]/(count - 1)   #for unbiasedness\n",
        "    print('goof')            \n",
        "    return sigma_1"
      ],
      "metadata": {
        "id": "-8zTJ35Ngh4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_sigma2(m, sigma0, sigma1, d):\n",
        "    #return (A) at page D, but for all i grouped together\n",
        "    '''print(m)\n",
        "    print(type(m))\n",
        "    print(m.shape)\n",
        "    print(type(m.shape))'''\n",
        "    \n",
        "\n",
        "\n",
        "    out = np.zeros(d)\n",
        "    j_indices = m == 1\n",
        "    print(j_indices)\n",
        "    print(type(j_indices))\n",
        "    sigma1_jj_inv = np.linalg.inv(sigma1[j_indices][:, j_indices])\n",
        "    \n",
        "    for i in range(d):\n",
        "        \n",
        "        sigma0_i_j = sigma0[i, j_indices].reshape(1, sum(j_indices))\n",
        "        \n",
        "        out[i] = sigma0[i,i] - np.matmul( np.matmul(sigma0_i_j, sigma1_jj_inv) , np.transpose(sigma0_i_j))\n",
        "    print('good')  \n",
        "    return out"
      ],
      "metadata": {
        "id": "Bkb3JNp7gEAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_mu2(m, sigma0, sigma1, d):\n",
        "    #return (..) in (B) at page D it returns a list of 1xsum(m) vectors\n",
        "    #m = np.array(m) #doesn't work\n",
        "    ''' print(m)\n",
        "    print(type(m))\n",
        "    print(m.shape)\n",
        "    print(type(m.shape))'''\n",
        "    \n",
        "\n",
        "\n",
        "    out = np.zeros((d, d))\n",
        "    j_indices = (m == 1)\n",
        "    \n",
        "    sigma1_jj_inv = np.linalg.inv(sigma1[j_indices][:, j_indices])\n",
        "    \n",
        "    for i in range(d):\n",
        "        \n",
        "        sigma0_i_j = sigma0[i, j_indices].reshape(1, sum(j_indices))\n",
        "        \n",
        "        out[i] = np.matmul(sigma0_i_j, sigma1_jj_inv)\n",
        "    print('goog')    \n",
        "    return out"
      ],
      "metadata": {
        "id": "O56EM13egnl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul(tensor1, tensor2):\n",
        "  tensor1 = tf.cast(tensor1, dtype='float64')\n",
        "  tensor2 = tf.cast(tensor2, dtype='float64')\n",
        "  return tf.matmul(tensor1, tensor2)"
      ],
      "metadata": {
        "id": "21tIoNHe28uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main thing"
      ],
      "metadata": {
        "id": "gY7eZs_shwHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gain (data_x, gain_parameters, priors):\n",
        "  '''Impute missing values in data_x\n",
        "  \n",
        "  Args:\n",
        "    - data_x: original data with missing values\n",
        "    - gain_parameters: GAIN network parameters:     #this is a dictionary\n",
        "      - batch_size: Batch size\n",
        "      - hint_rate: Hint rate\n",
        "      - alpha: Hyperparameter\n",
        "      - iterations: Iterations\n",
        "      \n",
        "  Returns:\n",
        "    - imputed_data: imputed data\n",
        "  '''\n",
        "  ''' priors:\n",
        "      priors['covariance'] is a dxd array\n",
        "      priors['mean'] is a d array\n",
        "      '''\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  # Define mask matrix\n",
        "  data_m = 1-np.isnan(data_x)\n",
        "   \n",
        "  \n",
        "  \n",
        "  \n",
        "  # System parameters\n",
        "  batch_size = gain_parameters['batch_size']\n",
        "  hint_rate = gain_parameters['hint_rate']\n",
        "  alpha = gain_parameters['alpha']\n",
        "  iterations = gain_parameters['iterations']\n",
        "  \n",
        "  # Other parameters\n",
        "  no, dim = data_x.shape\n",
        "  \n",
        "  # Hidden state dimensions\n",
        "  h_dim = int(dim)\n",
        "  \n",
        "  # Normalization\n",
        "  norm_data, norm_parameters = normalization(data_x)\n",
        "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
        "  \n",
        "  #NEW STUFF HERE\n",
        "  sigma1 = create_sigma_1(norm_data, data_m) \n",
        "  mu1 = np.nanmean(data_x, axis = 0)  #should be all zero (sanity check)\n",
        "  #ALSO, VOLENDO, MODIFY PRIORS USING NORM_PARAMETERS\n",
        "  sigma0 = priors['covariance']\n",
        "  mu0 = priors['mean']\n",
        "  \n",
        "  #CONVERT THEM TO TENSORS\n",
        "  sigma1 = tf.cast(tf.convert_to_tensor(sigma1), dtype = 'float32')\n",
        "  mu1 = tf.cast(tf.convert_to_tensor(mu1), dtype = 'float32')\n",
        "  sigma0 = tf.cast(tf.convert_to_tensor(sigma0), dtype = 'float32')\n",
        "  mu0 = tf.cast(tf.convert_to_tensor(mu0), dtype = 'float32')\n",
        "\n",
        "\n",
        "  ## GAIN architecture   \n",
        "  # Input placeholders\n",
        "  # Data vector\n",
        "  X = tf.placeholder(tf.float64, shape = [None, dim])     #changed to 64 !!!!!!!!!!!!\n",
        "  # Mask vector \n",
        "  M = tf.placeholder(tf.float64, shape = [None, dim])\n",
        "  # Hint vector\n",
        "  H = tf.placeholder(tf.float64, shape = [None, dim])\n",
        "  \n",
        "  # Discriminator variables\n",
        "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs   #credo x_tilde, h as in Algo 1\n",
        "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  D_W2 =  tf.Variable(xavier_init([h_dim, h_dim]))\n",
        "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  D_W3 =  tf.Variable(xavier_init([h_dim, dim]))\n",
        "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs   #the output dimension should be the same as the data (we are trying to replicate it)\n",
        "  \n",
        "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]   #set of params\n",
        "  \n",
        "  #Generator variables\n",
        "  # Data + Mask as inputs (Random noise is in missing components)\n",
        "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))\n",
        "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
        "  G_b2 =  tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  G_W3 =  tf.Variable(xavier_init([h_dim, dim]))\n",
        "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
        "  \n",
        "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
        "  \n",
        "  ## GAIN functions\n",
        "  # Generator\n",
        "  def generator(x,m):\n",
        "    # Concatenate Mask and Data\n",
        "    print(type(x))\n",
        "    print(type(m))\n",
        "    inputs = tf.cast(tf.concat(values = [x, m], axis = 1), dtype = 'float32')\n",
        "    print(type(inputs))\n",
        "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
        "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
        "    # MinMax normalized output\n",
        "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3)      #cambia qui!!! tipo elimina sigmoid\n",
        "    return G_prob\n",
        "      \n",
        "  # Discriminator\n",
        "  def discriminator(x, h):\n",
        "    # Concatenate Data and Hint\n",
        "    print(type(x))\n",
        "    \n",
        "    inputs = tf.cast(tf.concat(values = [tf.cast(x, dtype = 'float32'), tf.cast(h, dtype = 'float32')], axis = 1), dtype = 'float32')\n",
        "    print(type(inputs))\n",
        "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
        "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
        "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "    return D_prob\n",
        "  \n",
        "  def conditional(X, M, sigma0, sigma1, mu0, mu1):\n",
        "    M_mat = tf.concat(values = [M for i in range(dim)], axis = 1) \n",
        "    M_mat = tf.reshape(M_mat,  shape = [tf.shape(M_mat)[0], dim, dim])\n",
        "    M_mat = tf.cast(M_mat, dtype = 'float32')\n",
        "\n",
        "    muc = mu0 + tf.reshape( tf.matmul( tf.math.multiply( tf.cast(M_mat, dtype = 'float32'), tf.cast(sigma0, dtype = 'float32')), \\\n",
        "                          tf.matmul(tf.cast(tf.linalg.inv(sigma1), dtype = 'float32') , tf.cast(tf.transpose(X), dtype = 'float32'))) , shape = [tf.shape(M_mat)[0], dim])\n",
        "    #sigmac = sigma_diag - tf.reshape( tf.matmul( tf.math.multiply(M_mat, sigma0), tf.matmul(tf.linalg.inv(sigma1) , tf.transpose(matmul( tf.math.multiply(M_mat, sigma0)))) , shape = [tf.shape(M_mat)[0], dim])\n",
        "\n",
        "\n",
        "    #calculating sigmac here\n",
        "    temp = tf.matmul(  tf.math.multiply(M_mat, sigma0)    ,  tf.matmul(tf.linalg.inv(sigma1) , tf.transpose( tf.math.multiply(M_mat, sigma0) ) ) )\n",
        "    temp = tf.transpose(temp)\n",
        "    temp = sigma0 - temp\n",
        "\n",
        "    indices = [[i, i] for i in range(dim)]\n",
        "    temp = tf.gather_nd(tf.transpose(temp), indices, batch_dims=0, name=None)\n",
        "    temp = tf.transpose(temp)\n",
        "    sigmac = temp\n",
        "    \n",
        "    return (muc, sigmac)\n",
        "      \n",
        "      \n",
        "  \n",
        "  ## GAIN structure\n",
        "  # Generator\n",
        "  G_sample = generator(X, M)\n",
        "  print(G_sample)\n",
        "  print(type(G_sample))\n",
        " \n",
        "  # Combine with observed data\n",
        "  Hat_X = tf.cast(X, dtype = 'float32') * tf.cast(M, dtype = 'float32') + tf.cast(G_sample, dtype = 'float32') * tf.cast((1-M), dtype = 'float32')\n",
        "  print('hat')\n",
        "  print(Hat_X)\n",
        "  print(type(Hat_X))\n",
        "  #new here the vectors muc and sigmac\n",
        "  conditional_mu, conditional_sigma = conditional(X, M, sigma0, sigma1, mu0, mu1)   #should put X instead of Hat_X\n",
        "  print('cond_mu')\n",
        "  print(conditional_mu)\n",
        "  print(type(conditional_mu))\n",
        "  \n",
        "  \n",
        "  \n",
        "  # Discriminator\n",
        "  D_prob = discriminator(Hat_X, H)\n",
        "  \n",
        "  ## GAIN loss\n",
        "  D_loss_temp = -tf.reduce_mean(tf.cast(M, dtype = 'float32') * tf.cast(tf.log(D_prob + 1e-8), dtype = 'float32') \\\n",
        "                                + tf.cast((1-M), dtype = 'float32') * tf.cast(tf.log(1. - D_prob + 1e-8) , dtype = 'float32'))\n",
        "  \n",
        "  G_loss_temp = -tf.reduce_mean(tf.cast((1-M), dtype = 'float32') * tf.cast(tf.log(D_prob + 1e-8), dtype = 'float32'))\n",
        "  \n",
        "  MSE_loss = \\\n",
        "  tf.reduce_mean((tf.cast(M, dtype = 'float32') * tf.cast(X, dtype = 'float32') - tf.cast(M, dtype = 'float32') * tf.cast(G_sample, dtype = 'float32'))**2) / \\\n",
        "  tf.cast(tf.reduce_mean(M), dtype = 'float32')\n",
        "  \n",
        "  #ADD THIRD LOSS HERE\n",
        "  print('M')\n",
        "  print(M.shape)\n",
        "  print('Hat_X')\n",
        "  print(Hat_X.shape)\n",
        "  print('conditional_mu')\n",
        "  print(conditional_mu.shape)\n",
        "  print('conditional_sigma')\n",
        "  print(conditional_sigma.shape)\n",
        "  print('D_prob')\n",
        "  print(D_prob.shape)\n",
        "  \n",
        "  loss_3 = \\\n",
        "  -tf.reduce_mean( tf.cast((1-M), dtype = 'float32') * tf.cast( tf.exp(  - (tf.cast(Hat_X, dtype = 'float32') - tf.cast(conditional_mu, dtype = 'float32'))**2 \\\n",
        "                  / (2*tf.cast(conditional_sigma, dtype = 'float32'))) , dtype = 'float32')  )\n",
        "  print('loss_3')\n",
        "  print(loss_3.shape)\n",
        "\n",
        "  \"\"\"\n",
        "  j_index = M == 1\n",
        "  loss_3 = 0\"\"\"\n",
        "  \"\"\"for i in range(dim):\n",
        "      loss_3 += (1 - M[i])*tf.math.exp(-tf.math.pow( (Hat_X[i] - mu0[i] - \n",
        "                tf.tensordot( tf.convert_to_tensor(conditional_mu[i]), \n",
        "                    tf.boolean_mask(Hat_X[i], j_index) - tf.boolean_mask(mu1, j_index) ) ), 2) / (2*conditional_sigma[i])) \n",
        "        \"\"\"\n",
        "  \n",
        "  \n",
        "  D_loss = D_loss_temp\n",
        "  G_loss = G_loss_temp + alpha * MSE_loss  + loss_3   #as defined in the paper, add beta\n",
        "  \n",
        "  ## GAIN solver\n",
        "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
        "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
        "  \n",
        "  ## Iterations\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "   \n",
        "  # Start Iterations\n",
        "  for it in tqdm(range(iterations)):    \n",
        "      \n",
        "    # Sample batch\n",
        "    batch_idx = sample_batch_index(no, batch_size)\n",
        "    X_mb = norm_data_x[batch_idx, :]  \n",
        "    M_mb = data_m[batch_idx, :]     #tells you where data is missing (look top of script)\n",
        "    # Sample random vectors  \n",
        "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
        "    # Sample hint vectors\n",
        "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
        "    H_mb = M_mb * H_mb_temp\n",
        "    print('checkpoint 1')  \n",
        "    # Combine random vectors with observed vectors\n",
        "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
        "    print('checkpoint 2')  \n",
        "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
        "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
        "    print('checkpoint 3') \n",
        "    _, G_loss_curr, MSE_loss_curr = \\\n",
        "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
        "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
        "    print('checkpoint 4')        \n",
        "  ## Return imputed data      \n",
        "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
        "  M_mb = data_m\n",
        "  X_mb = norm_data_x          \n",
        "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
        "  print('checkpoint 5')     \n",
        "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
        "  print('checkpoint 6') \n",
        "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
        "  print('checkpoint 7') \n",
        "  # Renormalization\n",
        "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
        "  print('checkpoint 8') \n",
        "  # Rounding\n",
        "  imputed_data = rounding(imputed_data, data_x)  \n",
        "  print('checkpoint 9')        \n",
        "  return imputed_data"
      ],
      "metadata": {
        "id": "jv49z1W5hNff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "2vUv0o4eib1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "def data_loader (data_name, miss_rate):\n",
        "  '''Loads datasets and introduce missingness.\n",
        "  \n",
        "  Args:\n",
        "    - data_name: letter, spam, or mnist\n",
        "    - miss_rate: the probability of missing components\n",
        "    \n",
        "  Returns:\n",
        "    data_x: original data\n",
        "    miss_data_x: data with missing values\n",
        "    data_m: indicator matrix for missing components\n",
        "  '''\n",
        "  \n",
        "  # Load data\n",
        "  if data_name in ['letter', 'spam']:               #change here if you add more datasets\n",
        "    file_name = '/content/GAIN/data/'+data_name+'.csv'\n",
        "    data_x = np.loadtxt(file_name, delimiter=\",\", skiprows=1)\n",
        "  elif data_name == 'mnist':\n",
        "    (data_x, _), _ = mnist.load_data()\n",
        "    data_x = np.reshape(np.asarray(data_x), [60000, 28*28]).astype(float)\n",
        "  elif data_name == 'synthetic':\n",
        "    data_x = create_dataset(n = 10000, d = 10).astype(float)\n",
        "  # Parameters\n",
        "  no, dim = data_x.shape\n",
        "  \n",
        "  # Introduce missing data\n",
        "  data_m = binary_sampler(1-miss_rate, no, dim)\n",
        "  miss_data_x = data_x.copy()\n",
        "  miss_data_x[data_m == 0] = np.nan\n",
        "      \n",
        "  return data_x, miss_data_x, data_m"
      ],
      "metadata": {
        "id": "l3DKxYgOictK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main letter spam"
      ],
      "metadata": {
        "id": "wQCf5BzIima3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse"
      ],
      "metadata": {
        "id": "R4W2Xm5ninXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main (args):\n",
        "  '''Main function for UCI letter and spam datasets.\n",
        "  \n",
        "  Args:\n",
        "    - data_name: letter or spam\n",
        "    - miss_rate: probability of missing components\n",
        "    - batch:size: batch size\n",
        "    - hint_rate: hint rate\n",
        "    - alpha: hyperparameter\n",
        "    - iterations: iterations\n",
        "    \n",
        "  Returns:\n",
        "    - imputed_data_x: imputed data\n",
        "    - rmse: Root Mean Squared Error\n",
        "  '''\n",
        "  \n",
        "  data_name = args['data_name']\n",
        "  miss_rate = args['miss_rate']\n",
        "  \n",
        "  gain_parameters = {'batch_size': args['batch_size'],\n",
        "                     'hint_rate': args['hint_rate'],\n",
        "                     'alpha': args['alpha'],\n",
        "                     'iterations': args['iterations']}\n",
        "  \n",
        "  # Load data and introduce missingness\n",
        "  ori_data_x, miss_data_x, data_m = data_loader(data_name, miss_rate)    #CHANGE HERE FOR MY EXPERIMENTS\n",
        "  \n",
        "  #nNEW HERE\n",
        "  prior0 = np.zeros(ori_data_x.shape[1])\n",
        "  prior1 = np.identity(ori_data_x.shape[1])\n",
        "  priors = {'mean' : prior0, 'covariance' : prior1}\n",
        "  # Impute missing data\n",
        "  imputed_data_x = gain(miss_data_x, gain_parameters, priors)\n",
        "  \n",
        "  # Report the RMSE performance\n",
        "  rmse = rmse_loss (ori_data_x, imputed_data_x, data_m)\n",
        "  \n",
        "  print()\n",
        "  print('RMSE Performance: ' + str(np.round(rmse, 4)))\n",
        "  \n",
        "  return imputed_data_x, rmse\n"
      ],
      "metadata": {
        "id": "riHIoU1hin1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {'data_name': 'synthetic', 'miss_rate' : 0.2, 'batch_size' : 128, 'hint_rate' : 0.9, 'alpha' : 100, 'iterations' : 10}"
      ],
      "metadata": {
        "id": "mjk9KnF2kdxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rPgtQy05kemf",
        "outputId": "8beb7044-0b5b-4076-af24-2d002c401928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "goof\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "Tensor(\"Sigmoid_32:0\", shape=(?, 10), dtype=float32)\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "hat\n",
            "Tensor(\"add_207:0\", shape=(?, 10), dtype=float32)\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "cond_mu\n",
            "Tensor(\"add_208:0\", shape=(?, 10), dtype=float32)\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "<class 'tensorflow.python.framework.ops.Tensor'>\n",
            "M\n",
            "(?, 10)\n",
            "Hat_X\n",
            "(?, 10)\n",
            "conditional_mu\n",
            "(?, 10)\n",
            "conditional_sigma\n",
            "(?, 10)\n",
            "D_prob\n",
            "(?, 10)\n",
            "loss_3\n",
            "()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint 1\n",
            "checkpoint 2\n",
            "checkpoint 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1363\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1364\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1457\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) INVALID_ARGUMENT: In[0] and In[1] must have compatible batch dimensions: [128,10,10] vs. [10,10,128]\n\t [[{{node MatMul_161}}]]\n\t [[Neg_53/_25]]\n  (1) INVALID_ARGUMENT: In[0] and In[1] must have compatible batch dimensions: [128,10,10] vs. [10,10,128]\n\t [[{{node MatMul_161}}]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b3f47c9a9b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-d028c4e1ea66>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mpriors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'mean'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprior0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'covariance'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprior1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# Impute missing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mimputed_data_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiss_data_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# Report the RMSE performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-94ef515cc840>\u001b[0m in \u001b[0;36mgain\u001b[0;34m(data_x, gain_parameters, priors)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint 3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     _, G_loss_curr, MSE_loss_curr =     sess.run([G_solver, G_loss_temp, MSE_loss],\n\u001b[0;32m--> 229\u001b[0;31m              feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint 4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;31m## Return imputed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 971\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    972\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1194\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1374\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1375\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1397\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1399\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) INVALID_ARGUMENT: In[0] and In[1] must have compatible batch dimensions: [128,10,10] vs. [10,10,128]\n\t [[node MatMul_161\n (defined at <ipython-input-52-94ef515cc840>:130)\n]]\n\t [[Neg_53/_25]]\n  (1) INVALID_ARGUMENT: In[0] and In[1] must have compatible batch dimensions: [128,10,10] vs. [10,10,128]\n\t [[node MatMul_161\n (defined at <ipython-input-52-94ef515cc840>:130)\n]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node MatMul_161:\nIn[0] Mul_192:\t\nIn[1] MatMul_160:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-53-b3f47c9a9b7f>\", line 1, in <module>\n>>>     main(args)\n>>> \n>>>   File \"<ipython-input-11-d028c4e1ea66>\", line 33, in main\n>>>     imputed_data_x = gain(miss_data_x, gain_parameters, priors)\n>>> \n>>>   File \"<ipython-input-52-94ef515cc840>\", line 155, in gain\n>>>     conditional_mu, conditional_sigma = conditional(X, M, sigma0, sigma1, mu0, mu1)   #should put X instead of Hat_X\n>>> \n>>>   File \"<ipython-input-52-94ef515cc840>\", line 130, in conditional\n>>>     temp = tf.matmul(  tf.math.multiply(M_mat, sigma0)    ,  tf.matmul(tf.linalg.inv(sigma1) , tf.transpose( tf.math.multiply(M_mat, sigma0) ) ) )\n>>> \n\nInput Source operations connected to node MatMul_161:\nIn[0] Mul_192:\t\nIn[1] MatMul_160:\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-53-b3f47c9a9b7f>\", line 1, in <module>\n>>>     main(args)\n>>> \n>>>   File \"<ipython-input-11-d028c4e1ea66>\", line 33, in main\n>>>     imputed_data_x = gain(miss_data_x, gain_parameters, priors)\n>>> \n>>>   File \"<ipython-input-52-94ef515cc840>\", line 155, in gain\n>>>     conditional_mu, conditional_sigma = conditional(X, M, sigma0, sigma1, mu0, mu1)   #should put X instead of Hat_X\n>>> \n>>>   File \"<ipython-input-52-94ef515cc840>\", line 130, in conditional\n>>>     temp = tf.matmul(  tf.math.multiply(M_mat, sigma0)    ,  tf.matmul(tf.linalg.inv(sigma1) , tf.transpose( tf.math.multiply(M_mat, sigma0) ) ) )\n>>> \n\nOriginal stack trace for 'MatMul_161':\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-53-b3f47c9a9b7f>\", line 1, in <module>\n    main(args)\n  File \"<ipython-input-11-d028c4e1ea66>\", line 33, in main\n    imputed_data_x = gain(miss_data_x, gain_parameters, priors)\n  File \"<ipython-input-52-94ef515cc840>\", line 155, in gain\n    conditional_mu, conditional_sigma = conditional(X, M, sigma0, sigma1, mu0, mu1)   #should put X instead of Hat_X\n  File \"<ipython-input-52-94ef515cc840>\", line 130, in conditional\n    temp = tf.matmul(  tf.math.multiply(M_mat, sigma0)    ,  tf.matmul(tf.linalg.inv(sigma1) , tf.transpose( tf.math.multiply(M_mat, sigma0) ) ) )\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 1096, in op_dispatch_handler\n    return dispatch_target(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 3654, in matmul\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1591, in batch_mat_mul_v2\n    \"BatchMatMulV2\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 746, in _apply_op_helper\n    attrs=attr_protos, op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3705, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 2101, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n"
          ]
        }
      ]
    }
  ]
}